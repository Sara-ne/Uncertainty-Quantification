clusterByEntailment

class ClusterByEntailment:
    """
    Clusters images based on textual entailment of their descriptions.
    Uses hierarchical clustering.
    """
    def __init__(self, entailment_checker, threshold=0.7):
        self.entailment_checker = entailment_checker
        self.threshold = threshold

    def cluster_images(self, descriptions):
        """
        Clusters images based on how much their descriptions entail each other.
        :param descriptions: List of text descriptions.
        :return: List of cluster labels.
        """
        num_images = len(descriptions)
        entailment_matrix = np.zeros((num_images, num_images))

        # Compute pairwise entailment scores
        for i in range(num_images):
            for j in range(i + 1, num_images):
                # Check both directions of entailment
                score_ij = self.entailment_checker.check_entailment(descriptions[i], descriptions[j])
                score_ji = self.entailment_checker.check_entailment(descriptions[j], descriptions[i])

                # Only store scores if both directions of entailment are true
                if score_ij > 0 and score_ji > 0:
                    entailment_matrix[i, j] = score_ij
                    entailment_matrix[j, i] = score_ji
            #for j in range(i + 1, num_images):
                #score = self.entailment_checker.check_entailment(descriptions[i], descriptions[j])
                #entailment_matrix[i, j] = score
                #entailment_matrix[j, i] = score  # Symmetric

        # Convert to hierarchical clustering
        linkage_matrix = linkage(1 - entailment_matrix, method='average')
        labels = fcluster(linkage_matrix, t=self.threshold, criterion='distance')
        return labels

def print_cluster_descriptions(descriptions, labels):
    """
    Prints the text descriptions of images grouped by their cluster labels.
    :param descriptions: List of image descriptions.
    :param labels: Cluster labels for each description.
    """
    cluster_dict = {}

    for desc, label in zip(descriptions, labels):
        if label not in cluster_dict:
            cluster_dict[label] = []
        cluster_dict[label].append(desc)

    for cluster, descs in cluster_dict.items():
        print(f"\nCluster {cluster}:")
        for d in descs:
            print(f"  - {d}")



cluster confidence
import numpy as np

def cluster_confidence(kmeans_model, embeddings):
    distances = kmeans_model.transform(embeddings)  # Distance to each cluster center
    min_distances = np.min(distances, axis=1)  # Get distance to assigned cluster
    confidence = 1 - (min_distances / np.max(min_distances))  # Normalize
    return np.mean(confidence)

confidence_kmeans = cluster_confidence(kmeans_model, embeddings_kmeans)
print(f"K-Means Cluster Confidence: {confidence_kmeans}")





other approach for aleatoric and epsitemic uncertainty -> add in main method
# Step 1: Paraphrase the prompt
    paraphraser = PromptParaphraser()
    paraphrased_prompts = paraphraser.paraphrase(prompt[0], num_paraphrases=5)
    print("Prompt Variants:", paraphrased_prompts)

    candidate_labels = [
        "Noodles and broth", "Soft-boiled egg", "Chopsticks", "Steam rising", "Wodden texture"
    ]

    # Take one generated image at a time,
    # Use CLIP to compare it to the text embeddings of candidate labels,
    # Compute a softmax over cosine similarities → a probability distribution for that image. (using get_image_probability functin)
    probabilities = [embedder.get_image_probability(img, candidate_labels) for img in images]

    # Step 3: Compute uncertainty
    punc_uncertainty = uncertainty_calculator.calculate_punc(
        probabilities=probabilities,
        images=images,
        embedder=embedder,
        prompt_variants=paraphrased_prompts,
        generator=generator
    )

    # punc appraoch: extract concepts and compute aleatoric and epistemic (use already generated BLIP descriptions)

    print(f"Aleatoric Uncertainty: {punc_uncertainty['aleatoric_uncertainty']}")
    print(f"Epistemic Uncertainty: {punc_uncertainty['epistemic_uncertainty']}")

    total_uncertainty = uncertainty_calculator.compute_predictive_uncertainty(
        punc_uncertainty["aleatoric_uncertainty"],
        punc_uncertainty["epistemic_uncertainty"]
    )

    print(f"Predictive Uncertainty: {total_uncertainty}")


in punc uncertainty calculator:
# compute aleatoric and epistemic using slightly different approach
    def compute_aleatoric_uncertainty(self, probabilities):
        """
        Computes aleatoric uncertainty using entropy over predicted distributions.
        :param probabilities: List of probability distributions for each image.
        :return: Aleatoric uncertainty score.
        """
        return np.mean([entropy(prob + self.epsilon, base=2) for prob in probabilities])

    def compute_epistemic_uncertainty(self, prompt_variants, generator, embedder):
        """
        Computes epistemic uncertainty by generating images from paraphrased prompts
        and measuring the variance across their CLIP embeddings.

        :param prompt_variants: List of paraphrased prompts.
        :param generator: Instance of StableDiffusionGenerator.
        :param embedder: Instance of CLIPEmbeddingExtractor.
        :return: Epistemic uncertainty score.
        """
        all_embeddings = []

        for prompt in prompt_variants:
            images = generator.generate_images(prompt, num_images=1)
            embeddings = embedder.get_image_embeddings(images)
            all_embeddings.append(embeddings[0])  # Only one image per paraphrased prompt

        all_embeddings = np.stack(all_embeddings)
        variance = np.var(all_embeddings, axis=0)
        return np.mean(variance)

    def calculate_punc(self, probabilities, images, embedder, prompt_variants, generator):
        """
        Calculates both aleatoric and epistemic uncertainty.
        :param probabilities: Probability distributions per generated image.
        :param images: List of generated images.
        :param embedder: Instance of CLIPEmbeddingExtractor.
        :param prompt_variants: List of paraphrased prompts.
        :param generator: Instance of StableDiffusionGenerator.
        :return: Dictionary containing aleatoric and epistemic uncertainty.
        """
        aleatoric = self.compute_aleatoric_uncertainty(probabilities)
        epistemic = self.compute_epistemic_uncertainty(prompt_variants, generator, embedder)

        return {
            "aleatoric_uncertainty": aleatoric,
            "epistemic_uncertainty": epistemic
        }

    def compute_predictive_uncertainty(self, aleatoric_uncertainty, epistemic_uncertainty):
        return aleatoric_uncertainty + epistemic_uncertainty


ID_OOD with Random forest classifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
import argparse
import os


def main(args):
    # Load data
    df_id = pd.read_csv(args.id_csv)
    df_ood = pd.read_csv(args.ood_csv)

    # Label the datasets
    df_id['label'] = 0
    df_ood['label'] = 1

    # Optional: subsample to 50 each if needed
    df_id_sample = df_id.sample(n=args.n_samples, random_state=42)
    df_ood_sample = df_ood.sample(n=args.n_samples, random_state=42)

    # Combine
    df_combined = pd.concat([df_id_sample, df_ood_sample], ignore_index=True)

    # Define features
    features = [
        'Aleatoric_Uncertainty',
        'Epistemic_Uncertainty',
        'Predictive_Uncertainty',
        'CLIP_Score_Variance',
        'LPIPS_Diversity',
        'Semantic_Entropy_Embeddings',
        'Semantic_Entropy_Entailment'
    ]

    X = df_combined[features]
    y = df_combined['label']

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # Train classifier
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train, y_train)

    # Evaluate
    y_pred = clf.predict(X_test) # array of predicted class labels for test set (produced by classifier)
    y_proba = clf.predict_proba(X_test)[:, 1] # predictied prob that sample belongs to OOD

    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba) # with y_test= true binary label (0/1) and y_proba = predicted prob. that sample belongs to ID/OOD
    cm = confusion_matrix(y_test, y_pred)

    print(f"\n✅ Classification Results")
    print(f"Accuracy: {acc:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("Confusion Matrix:")
    print(cm)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--id_csv', type=str, required=True, help="Path to In-Distribution CSV file")
    parser.add_argument('--ood_csv', type=str, required=True, help="Path to Out-of-Distribution CSV file")
    parser.add_argument('--n_samples', type=int, default=50, help="Number of samples to draw from each (default=50)")
    args = parser.parse_args()

    main(args)


ID_OOD for only epistemic uncertainty
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
import argparse
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc


def main(args):
    # Load data
    df_id = pd.read_csv(args.id_csv)
    df_ood = pd.read_csv(args.ood_csv)

    # Label the datasets
    df_id['label'] = 0  # ID
    df_ood['label'] = 1  # OOD

    # subsample to N samples
    df_id_sample = df_id.sample(n=args.n_samples, random_state=42)
    df_ood_sample = df_ood.sample(n=args.n_samples, random_state=42)

    # Combine and shuffle
    df_combined = pd.concat([df_id_sample, df_ood_sample], ignore_index=True).sample(frac=1, random_state=42)

    # Feature and label
    X = df_combined[['Epistemic_Uncertainty']].values.flatten()
    y = df_combined['label'].values

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # Prediction based on threshold
    # Use mean of training set epistemic uncertainty as a threshold
    threshold = np.mean(X_train)
    y_pred = (X_test > threshold).astype(int)  # Predict OOD if uncertainty > threshold

    # Evaluate
    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, X_test)  # Use raw epistemic values as scores
    cm = confusion_matrix(y_test, y_pred)

    # Output
    print(f"\n Epistemic Uncertainty-Based Classification Results")
    print(f"Threshold used: {threshold:.4f}")
    print(f"Accuracy: {acc:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("Confusion Matrix:")
    print(cm)

    # Plot ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, X_test)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve Based on Epistemic Uncertainty')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.tight_layout()
    #plt.show()
    plt.savefig("roc_curve_epistemic.png")
    plt.close()
    print("ROC curve saved as 'roc_curve_epistemic.png'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--id_csv', type=str, required=True, help="Path to In-Distribution CSV file")
    parser.add_argument('--ood_csv', type=str, required=True, help="Path to Out-of-Distribution CSV file")
    parser.add_argument('--n_samples', type=int, default=50, help="Number of samples to draw from each (default=50)")
    args = parser.parse_args()

    main(args)
